**PR Title**
feat(narrative): switch creative generation to Hugging Face Inference API (no mocks); add prompt pack, retry/backoff; docs+tests

**⚠️ ARCHIVE NOTICE: This PR is HISTORICAL - superseded by Groq v1.6 integration. Do not use HF_TOKEN/HF_MODEL - use GROQ_API_KEY/GROQ_MODEL instead.**

---

**PR Body**
This PR standardizes **Narrative** creative generation on **Hugging Face Inference API**. There are **no mocks** and **no provider switches at runtime**. If `HF_TOKEN` is not set, the service fails fast. **Media** remains as implemented (your Phase-2 LM Studio work stays intact). Contracts and DB remain unchanged; all responses continue to use the **Envelope**.

* Hugging Face is the **sole** creative engine for Narrative (logline/plot, character bible, dialogue/scene).
* Prompt pack added for consistent, trope-aware outputs.
* Robust **retry/backoff** on 429/5xx.
* Docs updated (AGENTS.md, SRS acceptance).
* Integration tests provided; they **require** `HF_TOKEN` and are **skipped** if not present (no mocks).

---

## Patch (unified diffs)

```diff
*** Begin Patch
*** Update File: .env.example
@@
 OAUTH_ISSUER=http://localhost:8000
-#
-# --- Creative Scribe (Hugging Face) ---
-# Token from https://huggingface.co/settings/tokens
-HF_TOKEN=
-# Default model id for serverless Inference API (override in deploy)
-# Examples: HuggingFaceH4/zephyr-7b-beta  |  meta-llama/Llama-3.1-8B-Instruct
-HF_MODEL=HuggingFaceH4/zephyr-7b-beta
-# Optional: choose run mode. "serverless" (default) uses HF Inference API.
-# Set to "space" to call your own Hugging Face Space endpoint for bursts.
-CREATIVE_RUN_MODE=serverless
-# Optional: when set to "1", returns deterministic offline stubs (for CI).
-HF_OFFLINE_MOCK=0
+
+# --- Narrative Creative Generation (Hugging Face ONLY) ---
+# REQUIRED at runtime. Service fails fast if missing.
+HF_TOKEN=
+# REQUIRED model id for Hugging Face Inference API
+# Example: HuggingFaceH4/zephyr-7b-beta
+HF_MODEL=HuggingFaceH4/zephyr-7b-beta
*** End Patch
```

```diff
*** Begin Patch
*** Update File: requirements-dev.txt
@@
 jsonschema==4.23.0
 psycopg[binary]==3.2.1
+requests
*** End Patch
```

```diff
*** Begin Patch
*** Add File: docs/prompts/scribe_prompts.json
+{
+  "logline": {
+    "system": "You are a development executive who writes concise, high-signal loglines without clichés. Output 1–2 sentences and 3 hook bullets.",
+    "user": "SEED:\\n- Premise: {premise}\\n- Genre/tone: {genre}\\n- Comps: {comps}\\nCONSTRAINTS:\\n- Avoid: {trope_bans}\\nGOAL:\\n- Logline + 3 hooks with specific nouns/verbs."
+  },
+  "outline": {
+    "system": "You are a story architect. Produce {structure} beats. Each beat has: objective, turn (conflict or twist), value shift (axis: from→to), promises made, promises paid.",
+    "user": "CANON (facts only):\\n{canon}\\nREQUEST:\\n- Structure: {structure}\\n- Value axis: {axis}\\n- Avoid clichés: {trope_bans}\\nOUTPUT JSON: { beats[], notes[] }"
+  },
+  "scene": {
+    "system": "You are a senior TV staff writer. Write a cinematic scene (900–1200 words) with lean action lines, distinct voices, and a clear beat turn.",
+    "user": "WHERE: {where}\\nWHEN: {when}\\nWHO: {who}\\nGOAL: {goal}\\nOBSTACLES: {obstacles}\\nPREFER PAYOFF: {promise}\\nSTYLE: avoid clichés {trope_bans}; camera sense; subtext.\\nEND: change value axis {axis}.\\nRETURN: plain prose."
+  },
+  "rewrite": {
+    "system": "You are a script doctor. Reduce clichés without changing plot.",
+    "user": "INPUT SCENE:\\n{scene}\\nAVOID: {trope_bans}\\nKEEP: plot beats, voices, canon invariants.\\nREWRITE: tighten by 10–20%, remove vague intensifiers, show-don't-tell.\\nRETURN: new scene + 3-item change log."
+  },
+  "lineedit": {
+    "system": "You are a line editor. Neutral tone. No new facts.",
+    "user": "TEXT:\\n{text}\\nTASKS:\\n- Remove filler adverbs and vague intensifiers.\\n- Prefer active voice unless motivated.\\n- Balance names/pronouns.\\nRETURN: the revised text only."
+  },
+  "character_bible": {
+    "system": "You are a canon keeper. Summarize immutable traits and voice tells.",
+    "user": "SOURCES:\\n{snippets}\\nOUTPUT JSON: {\"id\":\"{char_id}\",\"immutable\":[\"...\"],\"lexicon\":{\"swears\":\"…\",\"metaphors\":\"…\"},\"tells\":{\"speech\":\"…\",\"action\":\"…\"}}"
+  }
+}
*** End Patch
```

```diff
*** Begin Patch
*** Add File: services/narrative/scribe/__init__.py
+# Creative Scribe package marker
*** End Patch
```

```diff
*** Begin Patch
*** Add File: services/narrative/scribe/hf_client.py
+from __future__ import annotations
+import os, json, time
+from typing import Any, Dict, Tuple
+import requests
+from services.narrative.ledger import compute_promise_payoff, trope_budget_ok
+
+PROMPT_FILES = [
+    os.path.join("docs", "prompts", "scribe_prompts.json"),
+    os.path.join(os.path.dirname(__file__), "scribe_prompts.json")
+]
+
+def _load_prompts() -> Dict[str, Dict[str, str]]:
+    for p in PROMPT_FILES:
+        if os.path.exists(p):
+            with open(p, "r", encoding="utf-8") as f:
+                return json.load(f)
+    raise RuntimeError("scribe_prompts.json not found")
+
+PROMPTS = _load_prompts()
+
+def _fill(t: str, vars: Dict[str, Any]) -> str:
+    try:
+        return t.format(**vars)
+    except KeyError:
+        return t
+
+def _render(task: str, vars: Dict[str, Any]) -> str:
+    spec = PROMPTS.get(task, {})
+    system = _fill(spec.get("system", ""), vars)
+    user = _fill(spec.get("user", ""), vars)
+    return f"SYSTEM:\\n{system}\\n\\nUSER:\\n{user}".strip()
+
+def _backoff(retry: int) -> float:
+    # 0.5, 1.0, 2.0, 4.0 ...
+    return 0.5 * (2 ** retry)
+
+def _hf_infer(model_id: str, token: str, prompt: str, controls: Dict[str, Any]) -> str:
+    url = f"https://api-inference.huggingface.co/models/{model_id}"
+    headers = {"Authorization": f"Bearer {token}"}
+    payload = {
+        "inputs": prompt,
+        "parameters": {
+            "max_new_tokens": int(controls.get("max_new_tokens", 1200)),
+            "temperature": float(controls.get("temperature", 0.7)),
+            "top_p": float(controls.get("top_p", 0.9)),
+            "return_full_text": False
+        }
+    }
+    for i in range(5):
+        r = requests.post(url, headers=headers, json=payload, timeout=120)
+        if r.status_code in (429, 500, 502, 503, 504):
+            time.sleep(_backoff(i))
+            continue
+        r.raise_for_status()
+        data = r.json()
+        if isinstance(data, list) and data and "generated_text" in data[0]:
+            return data[0]["generated_text"]
+        if isinstance(data, dict) and "generated_text" in data:
+            return data["generated_text"]
+        if isinstance(data, dict) and "choices" in data:
+            return data["choices"][0].get("text", "")
+        # Fallback to string
+        return str(data)
+    raise RuntimeError("Hugging Face inference failed after retries")
+
+def generate(task: str, inputs: Dict[str, Any], controls: Dict[str, Any] | None = None) -> Dict[str, Any]:
+    """
+    Sole creative generator for Narrative.
+    Returns {draft, model, provider, controls, issues}
+    """
+    token = os.environ.get("HF_TOKEN", "").strip()
+    model = os.environ.get("HF_MODEL", "").strip()
+    if not token or not model:
+        raise RuntimeError("HF_TOKEN and HF_MODEL are required")
+    controls = controls or {}
+    prompt = _render(task, inputs or {})
+    text = _hf_infer(model, token, prompt, controls)
+    out = {"draft": text, "model": model, "provider": "hf", "controls": controls}
+
+    # Narrative QA (if input provides hints)
+    banned = inputs.get("trope_bans", [
+        "chosen one","ancient prophecy","dark lord","it was all a dream",
+        "mysterious stranger","forbidden forest","destined to",
+        "balance of light and dark","last of their kind","prophecy foretold","bloodline power"
+    ])
+    ok_trope, counts = trope_budget_ok(out["draft"], banned=banned, max_per_1k=2)
+    issues = []
+    if not ok_trope:
+        issues.append({"type": "trope_budget", "counts": counts})
+    cards = inputs.get("cards")
+    if cards:
+        ledger = compute_promise_payoff(cards)
+        if ledger["orphans"]:
+            issues.append({"type":"promise_orphans","items":ledger["orphans"]})
+        if ledger["extraneous"]:
+            issues.append({"type":"promise_extraneous","items":ledger["extraneous"]})
+    out["issues"] = issues
+    return out
*** End Patch
```

```diff
*** Begin Patch
*** Update File: services/narrative/main.py
@@
-from services.narrative.ledger import compute_promise_payoff, trope_budget_ok
+from services.narrative.ledger import compute_promise_payoff, trope_budget_ok
+from services.narrative.scribe.hf_client import generate as hf_generate
+import os
@@
 class OutlineReq(BaseModel):
     world_id: str
     premise: str
     mode: str
     constraints: Optional[List[str]] = None
     draft_text: Optional[str] = None
     cards: Optional[List[SceneCard]] = None
@@
 def outline(req: OutlineReq):
@@
     return envelope_ok({"beats": beats, "issues": issues, "ledger": ledger}, {"actor":"ai","world_id": req.world_id})
+
+# -------- Narrative creative generation (HF ONLY) --------
+class GenReq(BaseModel):
+    task: str  # logline|outline|scene|rewrite|lineedit|character_bible|plot|character|dialogue
+    inputs: Dict[str, Any]
+    options: Optional[Dict[str, Any]] = None
+
+@app.post("/narrative/generate/plot")
+def gen_plot(req: GenReq):
+    out = hf_generate("logline", req.inputs, req.options)
+    return envelope_ok({"draft": out["draft"], "issues": out["issues"], "model": out["model"], "provider": out["provider"], "controls": out["controls"]}, {"actor":"ai"})
+
+@app.post("/narrative/generate/character")
+def gen_character(req: GenReq):
+    out = hf_generate("character_bible", req.inputs, req.options)
+    return envelope_ok({"draft": out["draft"], "issues": out["issues"], "model": out["model"], "provider": out["provider"], "controls": out["controls"]}, {"actor":"ai"})
+
+@app.post("/narrative/generate/dialogue")
+def gen_dialogue(req: GenReq):
+    out = hf_generate("scene", req.inputs, req.options)
+    return envelope_ok({"draft": out["draft"], "issues": out["issues"], "model": out["model"], "provider": out["provider"], "controls": out["controls"]}, {"actor":"ai"})
*** End Patch
```

```diff
*** Begin Patch
*** Update File: docs/SSOT/StoryMaker_SRS_v1.1.md
@@
 ## Acceptance (selected)
  - A1: DB health gate (WorldCore `/health` ok only when DB reachable)
  - A2: Approve idempotent
  - A3: Temporal guard sane (Allen-lite)
  - A4: Promise/Payoff ledger flags orphans
  - A5: Trope budget enforced (per 1k words)
  - A6: NPC WS proposes facts, never mutates canon
  - A7: Screenplay export returns artifact envelope
+ - A8: Narrative creative generation uses Hugging Face Inference API exclusively; service fails fast if `HF_TOKEN`/`HF_MODEL` not set.
*** End Patch
```

```diff
*** Begin Patch
*** Update File: AGENTS.md
@@
 @./docs/AGENTS.style.md
 @./docs/AGENTS.module.template.md
 @./docs/Cursor_System_Prompt.md
+
+## Narrative Creative Generation (HF ONLY)
+- Narrative calls **Hugging Face Inference API** with `HF_TOKEN` and `HF_MODEL`.
+- No mocks and no provider switching at runtime.
+- After generation, Narrative must run Trope Budget & Promise/Payoff and include issues in the Envelope.
*** End Patch
```

```diff
*** Begin Patch
*** Add File: tests/test_hf_integration_skip_if_no_token.py
+import os, pytest
+from services.narrative.scribe.hf_client import generate
+
+HF_TOKEN_PRESENT = bool(os.environ.get("HF_TOKEN", "").strip())
+
+@pytest.mark.skipif(not HF_TOKEN_PRESENT, reason="HF_TOKEN not set; real HF call required")
+def test_hf_scene_generation_smoke():
+    out = generate("scene", {
+        "where": "Harbor of Lumen",
+        "when": "night",
+        "who": "Elyra; Captain Rios",
+        "goal": "Inspect patrols before the storm",
+        "obstacles": "Thick fog; missing skiff",
+        "axis": "safety",
+        "trope_bans": ["chosen one","ancient prophecy","dark lord"]
+    }, {"temperature":0.8,"top_p":0.9,"max_new_tokens":400})
+    assert out["provider"] == "hf"
+    assert isinstance(out["draft"], str) and len(out["draft"]) > 0
+    assert isinstance(out["issues"], list)
*** End Patch
```

---

## Acceptance

* Narrative **only** uses Hugging Face; service refuses to start without `HF_TOKEN`/`HF_MODEL`.
* Media remains as implemented in Phase-2 (no interference).
* No mocks anywhere; tests that require HF are skipped if no token.
* Envelope preserved; issues (trope/promise) still returned.

## Quick test (live)

```bash
export HF_TOKEN=hf_xxx
export HF_MODEL=HuggingFaceH4/zephyr-7b-beta
make api.up
curl -s localhost:8001/narrative/generate/plot -H 'content-type: application/json' -d '{
  "task":"logline",
  "inputs":{"premise":"A harbor captain faces living fog.","genre":"nautical fantasy","comps":"The Terror x The Witcher","trope_bans":["chosen one","ancient prophecy"]},
  "options":{"temperature":0.6,"top_p":0.9,"max_new_tokens":180}
}' | jq
```

Merge when green.
